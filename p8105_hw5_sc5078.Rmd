---
title: "p8105_hw5_sc5078"
author: "Yvonne Chen"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(ggplot2)
library(p8105.datasets)
library(viridis)
```

# Problem 1
## Load dataset, create 'city_state' variable, and clean the dataset
The Washington Post has gathered data on homicides in 50 large U.S. cities:
```{r}
homicide = 
  read.csv("./data/homicide-data.csv") |>
  janitor::clean_names() |>
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = ifelse(disposition == "Closed by arrest", "solved", "unsolved")
    )
```
The dataset `homicide` has `r nrow(homicide)` cases, on variables that include the uid, reported date, victim name, race, age, and sex; the date the homicide was reported; the location of the homicide; and the status of the case. In order to clean the data, names of variables are uniform by perform `janitor::clean_names()`. `city_state` variable was created to show both city and state. `resolution` variable was created to indicate whether the case was solved.

## Summarize within cities to obtain the total number of homicides and the number of unsolved homicides
```{r}
homicide_city = 
homicide |>
  group_by(city_state) |>
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved")
    )
```

## Estimate the proportion of homicides that are unsolved for the city of Baltimore, MD
```{r}
prop.test(
  x = filter(homicide_city, city_state %in% "Baltimore, MD") |> pull(hom_unsolved),
  n = filter(homicide_city, city_state %in% "Baltimore, MD") |> pull(hom_total)
 ) |>
  broom::tidy() |>
  janitor::clean_names() |>
  select(estimate, conf_low, conf_high)
```

## Run prop.test for each of the cities
```{r warning=FALSE}
test_results = 
  homicide_city |> 
  mutate(
    prop_tests = purrr::map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = purrr::map(prop_tests, broom::tidy)) |>
  select(-prop_tests) |>
  unnest(tidy_tests) |>
  janitor::clean_names() |>
  select(city_state, estimate, conf_low, conf_high) |>
  mutate(city_state = fct_reorder(city_state, estimate))
```

## Create a plot that shows the estimates and CIs for each city
```{r}
test_results |>
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) +
  theme(axis.text.x = element_text(angle = 90))

```



# Problem 2
## Start with a dataframe containing all file names
```{r}
file_name = list.files(path = "./data/problem2", all.files = FALSE, full.names = FALSE)
list(file_name)
```

## Iterate over file names, read in data, and tidy the result
```{r}
select_name <- function(x) {
  subject_ID_x = substr(x, 5,6)
  arm_x = substr(x, 1, 3)
  
  tibble(
    subject_ID = subject_ID_x, 
    arm = arm_x
  )
}

sep_name <- select_name(file_name)

file_paths <- c("./data/con_01.csv", "./data/con_02.csv", "./data/con_03.csv", "./data/con_04.csv", "./data/con_05.csv", "./data/con_06.csv", "./data/con_07.csv", "./data/con_08.csv", "./data/con_09.csv", "./data/con_10.csv", "./data/exp_01.csv", "./data/exp_02.csv", "./data/exp_03.csv", "./data/exp_04.csv", "./data/exp_05.csv", "./data/exp_06.csv", "./data/exp_07.csv", "./data/exp_08.csv", "./data/exp_09.csv", "./data/exp_10.csv")

read_data <- function(x) {
  data <- read.csv(x)  
  return(data)
}

hw5_data<- purrr::map(file_paths, read_data) |>
  bind_rows() |>
  bind_cols(x=_, y=sep_name) |>
  janitor::clean_names() |>
    pivot_longer(
      week_1:week_8,
      names_to = "week",
      names_prefix = "week_",
      values_to = "observation")
```

## Make a spaghetti plot showing observations on each subject over time
```{r}
hw5_data |>
  group_by(arm) |>
  ggplot(aes(x = week, y = observation, group = subject_id, color = subject_id)) +
  geom_line() +
  facet_wrap(~ arm) +
  labs(title = "Spaghetti Plot for Control and Experiment Group", x = "Week", y = "Observation") +
  theme_minimal()
```
According to the spaghetti plot of control group, there isn't a clear pattern of observation change over time. Specifically, the observation value in control group is relative stable among subjects over time. However, in the experiment group, subjects have a general increasing pattern of observation values over time.


# Problem 3
## Construct dataset
```{r}
set.seed(6)

sim_mean = function(mu, n = 30, sigma = 5, alpha = 0.05, sim = 5000) {
  results <- data.frame(mu = rep(mu, sim),
                        mu_hat = numeric(sim),
                        p_value = numeric(sim))
for (i in 1:sim) {
  x <- rnorm(n, mean = mu, sd = sigma)
  
  t_result <- broom::tidy(t.test(x))
  
  results$mu_hat[i] <- mean(x)
  results$p_value[i] <- t_result$p.value
} 
  results$power <- mean(results$p_value < alpha)
  
  return(results)
}

mu_values <- c(0,1,2,3,4,5,6)

combined_results <- data.frame()

for (mu in mu_values) {
  current_results <- sim_mean(mu)

  combined_results <- rbind(combined_results, current_results)
}
```

## Make a plot showing the power of the test vs. the true value of μ
```{r}
combined_results |>
  ggplot(aes(x = mu, y = power))+
  geom_point() +
  geom_line() +
  labs(title = "Power of test vs. the true value of μ", x = "True value of μ", y = "Power of test") +
  theme_minimal()

```

The general relationship between the power of test and the true value of μ is that the larger the effect size, the larger the power. Specifically, power increase faster between the true mean 0 to 4, and slow down when the true mean increase from 4 to 6.

## Make a plot showing the average estimate of μ̂  vs. the true value of μ
```{r}
combined_results |>
  group_by(mu) |>
  mutate(
    mu_hat_avg = mean(mu_hat)
  ) |>
  ggplot(aes(x = mu, y = mu_hat_avg))+
  geom_point() +
  geom_line() +
  labs(title = "average estimate of μ_hat vs. the true value of μ", x = "True value of μ", y= "average estimate of μ_hat ") +
  theme_minimal()
```

## Make a plot of the average estimate of μ̂ only in samples for which the null was rejecte vs. the true value of μ
```{r}
combined_results |>
  group_by(mu) |>
  filter(p_value < 0.05) |>
  mutate(
    mu_hat_avg = mean(mu_hat)
  ) |>
  ggplot(aes(x = mu, y = mu_hat_avg))+
  geom_point() +
  geom_line() +
  labs(title = "average estimate of μ_hat vs. the true value of mu when the null was rejected", x = "True value of μ", y= "average estimate of μ_hat ") +
  theme_minimal()


```

According to two plots above, the sample average of μ̂  across tests for which the null is rejected is not equal to the true value of μ when μ is 1, 2, and 3 and they are approximately the same when μ is 0, 4, 5, and 6. The reason may be that when the true value of μ is small, the power of the test is weak and further influence the accuracy of estimation. 